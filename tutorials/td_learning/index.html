<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>TD Learning | Elena Sorina  Lupu</title>
    <meta name="author" content="Elena Sorina  Lupu">
    <meta name="description" content="website
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- MathJax Configuration -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://lupusorina.github.io/tutorials/td_learning/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Elena Sorina </span>Lupu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/robotics_art/">robotics &amp; art</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/tutorials/">tutorials &amp; lecture notes</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>

<p>I provide some notes on the paper. For a deeper understanding, I highly recommend reading the original work <a href="#references">[1]</a>, as these notes serve only as a concise summary highlighting the main ideas for didactic purposes. Some parts are quoted verbatim from the paper.</p>

<ul>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#methods">Methods</a></li>
  <li><a href="#td%CE%BB">TD(λ)</a></li>
  <li><a href="#theory-of-td-methods">Theory of TD methods</a></li>
  <li><a href="#related-research">Related research</a></li>
  <li><a href="#some-personal-views">Some personal views</a></li>
  <li><a href="#references">References</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>Temporal-difference learning methods are incremental learning procedures specialized for prediction problems.</p>

<p><strong>Conventional prediction-learning methods</strong> are driven by the error between predicted and actual outcome.<br>
<strong>Idea:</strong> we wait for the final outcome to compute the error.</p>

<p><strong>TD methods</strong> are driven by the error or difference between temporally successive predictions.<br>
<strong>Idea:</strong> we update before the final outcome is known.</p>

<h2 id="methods">Methods</h2>

<p>Let there be a sequence of observations 
\(x_1, x_2, \ldots, x_m,\)
with 
\(x_1, \ldots, x_m \in \mathbb{R}^n,\)
and let the outcome of the sequence be 
\(z \in \mathbb{R},\)
a scalar, and 
\(P_1, P_2, \ldots, P_m\)
are estimates of \(z,\)
also a function of \(w\) (i.e., \(P(x_t, w)\)), as shown in the figure below.</p>

<p><img src="../../assets/img/tutorials/td_learning/imgs/predictions.png" alt="Observations and predictions" width="400px" class="center"></p>

<p><strong>Rules for updating \(w\):</strong></p>
<ul>
  <li>for each observation, an increment to \(w\), denoted \(\Delta w\) is determined.</li>
  <li>after a complete sequence is processed, \(w\) is changed by the sum of all sequence increments</li>
</ul>

\[w = w + \sum_{t=1}^m \Delta w_t.\]

<p>In <strong>supervised learning</strong>, we treat each sequence of observations and its outcome as a sequence of observation-outcome pairs 
\((x_1, z), (x_2, z), \ldots, (x_m, z).\)</p>

<p>The optimization problem being solved is</p>

\[\mathcal{L} = \sum_{t=1}^m (P(w, x_t) - z)^2,\]

<p>As is standard in optimization, the parameter update relies on computing the gradient of this loss with respect to the model parameters \(w\) as follows</p>

\[\nabla_w \mathcal{L} = \sum_{t=1}^m 2 (P(w, x_t) - z) \nabla_w P(w, x_t),\]

<p>and using this gradient in the update rule</p>

\[\begin{aligned}
w &amp;= w - \alpha \nabla_w \mathcal{L}, \\
w &amp;= w + 2 (z - P(w, x_t)) \nabla_w P(w, x_t).
\end{aligned}\]

<p>Note that the update rule depends on \(z\), so we need to wait until the end of the sequence.</p>

<p>It is straightforward to derive the law above in case of linear functions (i.e., \(P(w, x_t) = w^\top x_t\). The increment in this case is</p>

\[\Delta w_t=\alpha\left(z-w^\top x_t\right) x_t,\]

<p>which is the Widrow-Hoff rule.</p>

<p><strong>Remark:</strong> If \(P_t\) is more complicated, like a DNN, we use the same rule, just that computing the quantity \(\nabla_w P\) is more involved. That’s where libraries like PyTorch are nice and useful.</p>

<p>There is an <strong>alternative</strong>: A TD procedure that produces the same result.<br>
We can represent the error</p>

\[z - P_t = \sum_{k=t}^m (P_{k+1} - P_k), \text{where } P_{m+1} = z.\]

<p>The relationship above is straightforward to see if we write the sum explicitly and cancel the terms.<br>
The rule is thus</p>

\[\Delta w_t=\alpha\left(P_{t+1}-P_t\right) \sum_{k=1}^t \nabla_w P_k.\]

<p>This law is called TD(1). The paper shows how this rule is derived.</p>

<p><strong>Proof:</strong><br>
We are following the proof as presented in the paper, with additional clarifications. The proof itself is straightforward, but careful attention to the indices is essential.</p>

\[\begin{aligned}
w &amp; \leftarrow w + \sum_{t=1}^m \alpha\left(z-P_t\right) \nabla_w P_t,  \\
w &amp; \leftarrow w + \sum_{t=1}^m \alpha \sum_{k=t}^m\left(P_{k+1}-P_k\right)\nabla_w P_t, \\
w &amp; \leftarrow w+\sum_{k=1}^m \alpha \sum_{t=1}^k\left(P_{k+1}-P_k\right) \nabla_w P_t,
\end{aligned}\]

<p>where in the last part we swapped the summation. A nice visualization of this summation is shown below.</p>

<p><img src="../../assets/img/tutorials/td_learning/imgs/sum_inversion.png" alt="Sum inversion trick" width="400px" class="center"></p>

<p>Then, the update rule becomes</p>

\[w \leftarrow w+\sum_{t=1}^m \alpha\left(P_{t+1}-P_t\right) \sum_{k=1}^t \nabla_w P_k.\]

<h2 id="tdlambda">TD(\(\lambda\))</h2>

<p>Considers a class of TD procedures that make greater alterations to more recent predictions</p>

\[\Delta w_t=\alpha\left(P_{t+1}-P_t\right) \sum_{k=1}^t \lambda^{t-k} \nabla_w P_k,\]

<p>where \(0\leq\lambda\leq1\).</p>

<h3 id="summary">Summary</h3>
<hr>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Update Rule</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>TD(0)</strong></td>
      <td>\(\Delta w_t=\alpha\left(P_{t+1}-P_t\right) \nabla_w P_t\)</td>
    </tr>
    <tr>
      <td><strong>TD(1)</strong></td>
      <td>\(\Delta w_t=\alpha\left(P_{t+1}-P_t\right) \sum_{k=1}^t \nabla_w P_k\)</td>
    </tr>
    <tr>
      <td><strong>TD(\(\lambda\))</strong></td>
      <td>\(\Delta w_t=\alpha\left(P_{t+1}-P_t\right) \sum_{k=1}^t \lambda^{t-k} \nabla_w P_k\)</td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="theory-of-td-methods">Theory of TD methods</h2>

<p>Two results: (1) an asymptotic convergence theorem for linear TD(0) when presented with new data sequences, (2) a theorem that linear TD(0) converges under repeated presentations to the optimal.<br>
The proofs use the Theory of Stochastic Approximations.</p>

<p>Initially, I intended to include more detailed notes on the proofs; however, as I am still assimilating the underlying theoretical framework, I chose to refrain from providing incomplete or potentially inaccurate commentary.</p>

<p>For the convergence proof, the theory is generated for data sequences generating by absorbing Markov processes, such as a random-walk process. “Absorbing” means that all sequences eventually terminate.<br>
We call \(T\) a set of terminal states, \(N\) a set of nonterminal states, and \(p_{ij}\) a set of transition probabilities.<br>
The state update is</p>

\[X_{t+1}= \begin{cases}X_t, &amp; X_t \in T \\ \text { random draw with } P\left(X_{t+1}=j \mid X_t=i\right)=p_{i j}, &amp; X_t=i \in N\end{cases}\]

<p>Below, I show a picture of such a process.</p>

<p><img src="../../assets/img/tutorials/td_learning/imgs/random_walk_absorbing.pdf" alt="Absorbing random walk" width="800px" class="center"></p>

<p>The proof shows that the expected values of the predictions found by linear TD(0) converge to the ideal predictions for data sequences generated by absorbing Markov processes.</p>

<p>The second part of the analysis examines which approach converges faster: temporal-difference (TD) methods or supervised learning. While both are guaranteed to converge asymptotically, the question is which one is faster? Answer: empirical evidence shows that TD converges faster.</p>

<p>One interesting result that I found is the following: “a common training process is to present a finite amount of data over and over again until the learning process converges. We prove that linear TD(0) converges under this repeated presentations training paradigm to the optimal predictions, while supervised-learning procedures converge to sub-optimal predictions. Since they are stepping toward a better final result, it makes sense that they would also be better after the first step.”</p>

<h2 id="related-research">Related research</h2>

<p>One aspect that I was wondering about while reading the paper is how does the theory extend outside episodic (finite) processes. For example, the paper shows the theory for an absorbing random walk (i.e., one that ends eventually).<br>
This is actually talked about in Section 6.4 in the paper.</p>

<p>Suppose you wish to predict the total return from investing in stock of various companies. The problem is that if a company never goes out of business, than the total return can be infinite (because it earns income every year). So typically, these infinite-horizon problems include some form of discounting factors.<br>
We want \(P_t\) to predict the discounted sum</p>

\[P_t=\sum_{k=0}^{\infty} \gamma^k c_{t+k+1} = c_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k c_{t+k+2} = c_{t+1} + \gamma P_{t+1}\]

<p>The TD error is \(\left(c_{t+1}+\gamma P_{t+1}\right)-P_t\). I am sure this is familiar to researchers doing deep RL.</p>

<h2 id="some-personal-views">Some personal views</h2>

<p>Reading this paper helped clarify several points that I was somewhat familiar with but had not fully internalized. For example, I now see more clearly that PPO uses TD learning for estimating the advantage function (thanks to Kris for pointing this out).</p>

<p>I would like to explore more rigorously how TD learning relates to adaptive control, since both frameworks rely on iterative update laws.</p>

<h2 id="references">References</h2>

<p>[1] R. S. Sutton, “Learning to Predict by the Methods of Temporal Differences,” Machine Learning, 1988.</p>

<p>[2] R. S. Sutton and A. G. Barto, “Reinforcement Learning: An Introduction,” MIT Press, 2018.</p>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Elena Sorina  Lupu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.
Last updated: August 18, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
